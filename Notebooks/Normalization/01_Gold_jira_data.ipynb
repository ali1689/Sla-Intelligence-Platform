{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e9db82-dc5a-4497-8fcb-69f539a61e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "GOLD_DB = \"slainte_gold\"\n",
    "DIM_PRIORITY = f\"{GOLD_DB}.dim_priority\"\n",
    "priority_data = [\n",
    "   (\"High\",   4.0, 1),\n",
    "   (\"Medium\", 8.0, 2),\n",
    "   (\"Low\",   72.0, 3),\n",
    "]\n",
    "dim_priority_df = spark.createDataFrame(\n",
    "   priority_data,\n",
    "   [\"priority\", \"target_hours\", \"priority_rank\"]\n",
    ")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {GOLD_DB}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DIM_PRIORITY}\")\n",
    "(\n",
    "   dim_priority_df\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .saveAsTable(DIM_PRIORITY)\n",
    ")\n",
    "print(f\"✅ dim_priority created: {DIM_PRIORITY}\")\n",
    "spark.table(DIM_PRIORITY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c68b5b-ed6b-4c10-9d28-14aa006e9610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DIM_ASSIGNEE = f\"{GOLD_DB}.dim_assignee_team\"\n",
    "assignee_data = [\n",
    "   (\"Alice Martin\",  \"Support Team\"),\n",
    "   (\"Bob Johnson\",   \"Support Team\"),\n",
    "   (\"Sarah Connor\",  \"Platform Team\"),\n",
    "   (\"John Smith\",    \"Platform Team\"),\n",
    "]\n",
    "dim_assignee_df = spark.createDataFrame(\n",
    "   assignee_data,\n",
    "   [\"assignee\", \"team\"]\n",
    ")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DIM_ASSIGNEE}\")\n",
    "(\n",
    "   dim_assignee_df\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .saveAsTable(DIM_ASSIGNEE)\n",
    ")\n",
    "print(f\"✅ dim_assignee_team created: {DIM_ASSIGNEE}\")\n",
    "spark.table(DIM_ASSIGNEE).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd72b8f8-58f7-45fc-a687-8737d8abc490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SILVER_DB = \"slainte_silver\"\n",
    "SILVER_TABLE = f\"{SILVER_DB}.jira_tickets_silver\"\n",
    "FACT_TABLE = f\"{GOLD_DB}.fact_tickets_sla\"\n",
    "silver = spark.table(SILVER_TABLE)\n",
    "dim_priority = spark.table(f\"{GOLD_DB}.dim_priority\")\n",
    "fact_df = (\n",
    "   silver\n",
    "   .filter(F.lower(F.col(\"status\")) == \"closed\")\n",
    "   .join(dim_priority, on=\"priority\", how=\"left\")\n",
    "   .withColumn(\n",
    "       \"resolution_breach\",\n",
    "       F.col(\"resolution_hours\") > F.col(\"target_hours\")\n",
    "   )\n",
    "   .withColumn(\n",
    "       \"breach_hours\",\n",
    "       F.when(\n",
    "           F.col(\"resolution_hours\") > F.col(\"target_hours\"),\n",
    "           F.col(\"resolution_hours\") - F.col(\"target_hours\")\n",
    "       ).otherwise(F.lit(0.0))\n",
    "   )\n",
    "   .select(\n",
    "       \"ticket_id\",\n",
    "       \"priority\",\n",
    "       \"priority_rank\",\n",
    "       \"target_hours\",\n",
    "       \"resolution_hours\",\n",
    "       \"resolution_breach\",\n",
    "       \"breach_hours\",\n",
    "       \"created_at\",\n",
    "       \"resolved_at\",\n",
    "       \"assignee\",\n",
    "       \"project_key\"\n",
    "   )\n",
    ")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {FACT_TABLE}\")\n",
    "(\n",
    "   fact_df\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .saveAsTable(FACT_TABLE)\n",
    ")\n",
    "print(f\"✅ fact_tickets_sla created: {FACT_TABLE}\")\n",
    "spark.table(FACT_TABLE).show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Gold_jira_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
